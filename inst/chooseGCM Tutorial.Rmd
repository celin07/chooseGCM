---
title: "chooseGCM Tutorial"
author: "Lu√≠z Fernando Esser"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(chooseGCM)
```


## Downloading WorldClim 2.1 data

First, we need to use only one time period. Here we use 2090 so the difference between models is more conspicuous. In the same way we are considering the SSP585, which is the more dramatic pathway. The resolution is the lowest to be quicker. The aim here is to maintain all parameters equal, but General Circulation Models (GCMs). In this way we know that the only source of variation comes from them. Note that if you receive a timeout error you can increase timeout value by running `r options(timeout = 6000)`, where 600 is the value in seconds that will be enough to download the data.

```{r WorldClim_data}
gcm_names <- sort(c('ac','ip','me','mi','mp','ml'))

# WorldClim_data(period = 'future', variable = 'bioc', year = '2090', gcm = gcm_names, ssp = '585', resolution = 10)
```


## Importing and transforming data

Now let's import GCMs to R in a list of stacks and name the list with the names of the GCMs.

```{r import_data}
s <- list.files('input_data/WorldClim_data_future', pattern = '.tif', full.names = T) |>
  lapply(function(x){s <- raster::stack(x)
                     names(s) <- paste0('bio_',1:19) # Rename rasters
                     return(s)})
names(s) <- gcm_names
```


In each function, data will be transformed. To do that you will always need to provide at least: (1) the list of stacks, (2) the variables you want to use in analysis and (3) the shapefile of your study area. 

```{r transform_gcms}
var_names <- c('bio_1', 'bio_12')
study_area <- raster::extent(c(-57, -22, -48, -33)) # you can use a shapefile here
```


## Exploratory analysis

In chooseGCM we implemented functions to analyze GCMs attributes.

```{r exploratory_analysis}
# Summary of GCMs
s_sum <- summary_gcms(s, var_names, study_area)
s_sum
# Pearson Correlation between GCMs
s_cor <- cor_gcms(s, var_names, study_area, method = "pearson")
s_cor
```


## Obtain clusters

Clusters in chooseGCM are obtained through k-means, a unsupervised machine learning algorithm. k is the number of clusters, which in this case is the number of GCMs the modeller wants to use in projections.To build a distance matrix considering multiple variables to each GCM we use a flattening strategy, where values are concatenated in one unique vector to each GCM. In the process, we need to scale variables so they end up with the same measure. This matrix will be used to calculate the clusters.

```{r kmeans_gcms}
kmeans_gcms(s, var_names, study_area, k = 3,  method = "euclidean")
```


We can also obtain clusters through hierarchical clustering.

```{r hclust_gcms}
hclust_gcms(s, var_names, study_area, k = 3, n = 1000)
```


But how many clusters are good? There is metrics to understand that.

```{r optimize_clusters}
optimize_clusters(s, var_names, study_area, method = 'wss', n = 1000)
optimize_clusters(s, var_names, study_area, method = 'silhouette', n = 1000)
#optimize_clusters(s_flat, method = 'gap_stat')
```


## Putting everything together

There is the option to run each function in a separate to better understand what is happening and to better parameterize each step. However there is a wrapper to help run everything at once.

```{r wrapper}
compare_gcms(s, var_names, study_area, k = 3)
```

